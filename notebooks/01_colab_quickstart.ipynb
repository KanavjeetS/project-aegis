{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Project A.E.G.I.S. - Quick Start\n",
                "\n",
                "**Autonomous Embedding-Guided Intelligence System**\n",
                "\n",
                "Run V-JEPA + VLM for disaster prediction entirely in Colab (no local storage needed!)\n",
                "\n",
                "---\n",
                "\n",
                "## Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Clone repository\n",
                "!git clone https://github.com/yourusername/project-aegis.git\n",
                "%cd project-aegis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies (full install works in Colab - 100GB+ storage!)\n",
                "!pip install torch torchvision transformers accelerate bitsandbytes\n",
                "!pip install timm einops omegaconf tqdm decord opencv-python"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Verify installation\n",
                "from models.vjepa import VJEPAModel\n",
                "from models.vlm import AEGISModel\n",
                "print('✅ Installation successful!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option 1: Test V-JEPA (Without Training)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "\n",
                "# Create V-JEPA model\n",
                "model = VJEPAModel(\n",
                "    img_size=224,\n",
                "    patch_size=16,\n",
                "    embed_dim=768,\n",
                "    depth=12,\n",
                ")\n",
                "\n",
                "print(f'Model created: {sum(p.numel() for p in model.parameters())/1e6:.1f}M parameters')\n",
                "\n",
                "# Test forward pass\n",
                "x = torch.randn(2, 3, 224, 224)  # 2 images\n",
                "predicted, target, mask = model(x)\n",
                "\n",
                "print(f'✅ Forward pass successful!')\n",
                "print(f'Predicted shape: {predicted.shape}')\n",
                "print(f'Target shape: {target.shape}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option 2: Train Q-Former (VLM)\n",
                "\n",
                "**Note:** This requires a pre-trained V-JEPA checkpoint. For demo purposes, we'll show the training loop structure."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Demo: Training loop structure (without actual data)\n",
                "!python scripts/train_vlm.py --config configs/vlm_config.yaml\n",
                "\n",
                "# This will show the training interface even without wandb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Option 3: Inference Demo\n",
                "\n",
                "Test embedding extraction (no training needed)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Extract embeddings from random video\n",
                "video = torch.randn(1, 16, 3, 224, 224)  # 1 video, 16 frames\n",
                "\n",
                "with torch.no_grad():\n",
                "    embeddings = model.extract_embeddings(video)\n",
                "\n",
                "print(f'✅ Embeddings extracted!')\n",
                "print(f'Shape: {embeddings.shape}')  # [1, 16, 768]\n",
                "print(f'Mean: {embeddings.mean():.4f}')\n",
                "print(f'Std: {embeddings.std():.4f}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Next Steps\n",
                "\n",
                "1. **Download V-JEPA checkpoint** from Meta FAIR\n",
                "2. **Upload your disaster videos** to Colab\n",
                "3. **Train Q-Former** for 6-8 hours on free T4\n",
                "4. **Deploy** using ONNX export\n",
                "\n",
                "---\n",
                "\n",
                "**Resources:**\n",
                "- [Project README](README.md)\n",
                "- [Training Guide](docs/TRAINING.md)\n",
                "- [Deployment Guide](docs/DEPLOYMENT.md)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}