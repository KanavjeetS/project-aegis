# VLM (Vision-Language Model) Configuration

model:
  # V-JEPA Encoder (frozen)
  vjepa_checkpoint: "checkpoints/vjepa_vitl16.pth"
  freeze_vjepa: true
  
  # Q-Former Configuration
  qformer:
    num_query_tokens: 32
    hidden_size: 768
    num_hidden_layers: 6
    num_attention_heads: 12
    intermediate_size: 3072
  
  # LLM Configuration (Llama 3.1 8B)
  llm:
    model_name: "meta-llama/Llama-3.1-8B-Instruct"
    quantization: "4bit"  # Use 4-bit quantization for low VRAM
    freeze_llm: true
    max_length: 512

training:
  batch_size: 4  # Small batch due to LLM memory requirements
  num_epochs: 10
  learning_rate: 0.0001
  weight_decay: 0.05
  warmup_steps: 500
  gradient_accumulation_steps: 4  # Effective batch size = 16
  max_grad_norm: 1.0

optimizer:
  type: AdamW
  betas: [0.9, 0.95]

scheduler:
  type: cosine
  min_lr: 1.0e-6

data:
  num_workers: 2
  video_caption_pairs: "data/video_captions.json"
  video_length: 16
  frame_sample_rate: 2

checkpoint:
  save_freq: 1
  keep_last_n: 3

logging:
  wandb: true
  log_freq: 50
  project_name: "project-aegis-vlm"
